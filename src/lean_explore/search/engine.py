"""Core search engine functionality for Lean declarations.

This module provides the core search functionality using SQLite for storage,
FAISS for semantic search, combined with BM25 lexical matching.

Note: On macOS, torch and FAISS have OpenMP library conflicts. To avoid segfaults:
- FAISS is imported lazily (not at module level)
- When semantic search is needed, torch/embeddings are loaded FIRST, then FAISS
"""

import difflib
import json
import logging
import re
from pathlib import Path
from typing import TYPE_CHECKING

import bm25s
import numpy as np
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncEngine, AsyncSession, create_async_engine

from lean_explore.config import Config
from lean_explore.models import Declaration, SearchResult

if TYPE_CHECKING:
    import faiss

    from lean_explore.util import EmbeddingClient, RerankerClient

logger = logging.getLogger(__name__)

EPSILON = 1e-9

def _is_autogenerated(name: str) -> bool:
    """Check if a declaration name is auto-generated by Lean.

    Auto-generated declarations include:
    - .mk constructors (e.g., Nat.mk)

    Args:
        name: Fully qualified declaration name.

    Returns:
        True if the declaration is auto-generated.
    """
    return name.endswith(".mk")


def _tokenize_spaced(text: str) -> list[str]:
    """Tokenize text with spacing on dots, underscores, and camelCase.

    Args:
        text: Input text to tokenize.

    Returns:
        List of lowercase word tokens.
    """
    if not text:
        return []
    # Replace dots and underscores with spaces
    text = text.replace(".", " ").replace("_", " ")
    # Split camelCase: insert space before uppercase letters
    text = re.sub(r"([a-z])([A-Z])", r"\1 \2", text)
    # Extract words
    return re.findall(r"\w+", text.lower())


def _tokenize_raw(text: str) -> list[str]:
    """Tokenize text as single token (preserves dots).

    Args:
        text: Input text to tokenize.

    Returns:
        List with the full text as a single lowercase token.
    """
    if not text:
        return []
    return [text.lower()]


def _fuzzy_name_score(query: str, name: str) -> float:
    """Compute fuzzy match score between query and declaration name.

    Normalizes both strings (dots/underscores -> spaces) and uses
    SequenceMatcher ratio for character-level similarity.

    Args:
        query: Search query string.
        name: Declaration name to match against.

    Returns:
        Similarity score between 0 and 1.
    """
    # Normalize: replace dots and underscores with spaces, lowercase
    norm_query = query.lower().replace(".", " ").replace("_", " ")
    norm_name = name.lower().replace(".", " ").replace("_", " ")
    return difflib.SequenceMatcher(None, norm_query, norm_name).ratio()


def _normalize_scores(scores: list[float]) -> list[float]:
    """Min-max normalize scores to [0, 1] range.

    Args:
        scores: List of raw scores.

    Returns:
        List of normalized scores.
    """
    if not scores:
        return []

    min_score = min(scores)
    max_score = max(scores)
    score_range = max_score - min_score

    if score_range < EPSILON:
        # All scores are the same
        if max_score > EPSILON:
            return [1.0] * len(scores)
        return [0.0] * len(scores)

    return [(s - min_score) / score_range for s in scores]


def _normalize_deps_log(counts: list[int]) -> list[float]:
    """Log-scale normalization for dependency counts.

    Uses log(1 + count) / log(1 + max_count) to compress the range
    and give more credit to items with moderate dependency counts.

    Args:
        counts: List of dependency counts.

    Returns:
        List of normalized scores in [0, 1] range.
    """
    import math

    if not counts:
        return []

    max_count = max(counts)
    if max_count == 0:
        return [0.0] * len(counts)

    log_max = math.log(1 + max_count)
    return [math.log(1 + c) / log_max for c in counts]


def _compute_ranks(scores: list[float]) -> list[int]:
    """Compute ranks for a list of scores (1-indexed, higher score = lower rank).

    Candidates with score 0 get rank len(scores)+1 (worst possible).

    Args:
        scores: List of raw scores.

    Returns:
        List of ranks (1 = best).
    """
    n = len(scores)
    # Create (index, score) pairs and sort by score descending
    indexed = [(i, s) for i, s in enumerate(scores)]
    indexed.sort(key=lambda x: x[1], reverse=True)

    ranks = [0] * n
    for rank, (idx, score) in enumerate(indexed, 1):
        if score > 0:
            ranks[idx] = rank
        else:
            # Zero score = not retrieved by this signal, assign worst rank
            ranks[idx] = n + 1

    return ranks


def _reciprocal_rank_fusion(rank_lists: list[list[int]], k: int = 0) -> list[float]:
    """Compute RRF scores from multiple rank lists.

    RRF(d) = sum(1 / (k + rank_i(d)) for each signal i)

    Args:
        rank_lists: List of rank lists, one per signal.
        k: Constant to prevent top rank from dominating. Default 0 means 1/rank.

    Returns:
        List of RRF scores for each candidate.
    """
    n = len(rank_lists[0])
    rrf_scores = []

    for i in range(n):
        score = sum(1.0 / (k + ranks[i]) for ranks in rank_lists)
        rrf_scores.append(score)

    return rrf_scores


def _weighted_score_fusion(
    score_lists: list[list[float]],
    weights: list[float],
) -> list[float]:
    """Combine multiple score lists using weighted normalized scores.

    Each score list is normalized to [0, 1] using min-max scaling,
    then combined with the given weights.

    Args:
        score_lists: List of score lists, one per signal.
        weights: Weight for each signal (should sum to 1.0 for interpretability).

    Returns:
        List of combined scores for each candidate.
    """
    if not score_lists:
        return []

    n = len(score_lists[0])
    if n == 0:
        return []

    # Normalize each score list
    normalized_lists = [_normalize_scores(scores) for scores in score_lists]

    # Compute weighted sum
    combined = []
    for i in range(n):
        score = sum(w * normalized_lists[j][i] for j, w in enumerate(weights))
        combined.append(score)

    return combined


class SearchEngine:
    """Core search engine for Lean declarations.

    Uses two-stage retrieval:
    1. FAISS semantic search on informalizations
    2. BM25 lexical search on declaration names (independent)
    Then merges and reranks candidates.
    """

    def __init__(
        self,
        db_url: str | None = None,
        embedding_client: "EmbeddingClient | None" = None,
        embedding_model_name: str = "Qwen/Qwen3-Embedding-0.6B",
        reranker_client: "RerankerClient | None" = None,
        reranker_model_name: str = "Qwen/Qwen3-Reranker-0.6B",
        faiss_index_path: Path | None = None,
        faiss_ids_map_path: Path | None = None,
        use_local_data: bool = True,
    ):
        """Initialize the search engine.

        Args:
            db_url: Database URL. Defaults to configured URL.
            embedding_client: Client for generating embeddings. Created lazily if None.
            embedding_model_name: Name of the embedding model to use.
            reranker_client: Client for reranking results. Created lazily if None.
            reranker_model_name: Name of the reranker model to use.
            faiss_index_path: Path to FAISS index. Defaults to config path.
            faiss_ids_map_path: Path to FAISS ID mapping. Defaults to config path.
            use_local_data: If True, use DATA_DIRECTORY paths. If False, use
                CACHE_DIRECTORY paths (for downloaded remote data).
        """
        # Store for lazy embedding client creation
        self._embedding_client = embedding_client
        self._embedding_model_name = embedding_model_name

        # Store for lazy reranker client creation
        self._reranker_client = reranker_client
        self._reranker_model_name = reranker_model_name

        # Select paths based on data source
        if use_local_data:
            base_path = Config.ACTIVE_DATA_PATH
            default_db_url = Config.EXTRACTION_DATABASE_URL
        else:
            base_path = Config.ACTIVE_CACHE_PATH
            default_db_url = Config.DATABASE_URL

        self.db_url = db_url or default_db_url
        self.engine: AsyncEngine = create_async_engine(self.db_url)

        # Store paths for lazy FAISS loading (to avoid torch/FAISS OpenMP conflict)
        # Informalization FAISS index (only index used for semantic search)
        self._faiss_informal_path = faiss_index_path or (
            base_path / "informalization_faiss.index"
        )
        self._faiss_informal_ids_path = faiss_ids_map_path or (
            base_path / "informalization_faiss_ids_map.json"
        )
        self._faiss_informal_index: faiss.Index | None = None
        self._faiss_informal_id_map: list[int] | None = None

        # BM25 indices (lazy loaded) - used for name matching
        self._all_declaration_ids: list[int] | None = None
        self._all_declaration_names: list[str] | None = None
        self._bm25_name_spaced: bm25s.BM25 | None = None
        self._bm25_name_raw: bm25s.BM25 | None = None

        # Validate paths exist (fail fast)
        for path in [self._faiss_informal_path, self._faiss_informal_ids_path]:
            if not path.exists():
                raise FileNotFoundError(
                    f"Required file not found at {path}. "
                    "Please run 'lean-explore download' to fetch the data."
                )

    @property
    def embedding_client(self) -> "EmbeddingClient":
        """Lazily create the embedding client to avoid loading torch at import time.

        This prevents OpenMP library conflicts between torch and FAISS on macOS.
        The embedding client (and torch) must be loaded BEFORE FAISS.
        """
        if self._embedding_client is None:
            from lean_explore.util import EmbeddingClient

            self._embedding_client = EmbeddingClient(
                model_name=self._embedding_model_name,
                max_length=512,
            )
        return self._embedding_client

    @property
    def reranker_client(self) -> "RerankerClient":
        """Lazily create the reranker client to avoid loading torch at import time."""
        if self._reranker_client is None:
            from lean_explore.util import RerankerClient

            self._reranker_client = RerankerClient(
                model_name=self._reranker_model_name,
                max_length=256,  # Prioritize quality
            )
        return self._reranker_client

    def _ensure_faiss_loaded(self) -> None:
        """Load the FAISS index if not already loaded.

        Important: This must be called AFTER torch is loaded (via embedding_client)
        to avoid OpenMP library conflicts on macOS.
        """
        if self._faiss_informal_index is not None:
            return

        import faiss

        # Load informalization FAISS index (only index used for semantic search)
        logger.info(f"Loading FAISS index from {self._faiss_informal_path}")
        self._faiss_informal_index = faiss.read_index(str(self._faiss_informal_path))
        with open(self._faiss_informal_ids_path) as f:
            self._faiss_informal_id_map = json.load(f)

    @property
    def faiss_informal_index(self) -> "faiss.Index":
        """Get the informalization FAISS index."""
        self._ensure_faiss_loaded()
        return self._faiss_informal_index  # type: ignore[return-value]

    @property
    def faiss_informal_id_map(self) -> list[int]:
        """Get the informalization FAISS ID mapping."""
        self._ensure_faiss_loaded()
        return self._faiss_informal_id_map  # type: ignore[return-value]

    async def _ensure_bm25_loaded(self) -> None:
        """Load BM25 indices over all declaration names."""
        if self._bm25_name_spaced is not None:
            return

        logger.info("Loading all declarations for BM25 indices...")
        async with AsyncSession(self.engine) as session:
            stmt = select(Declaration.id, Declaration.name)
            result = await session.execute(stmt)
            rows = result.all()

        self._all_declaration_ids = [row[0] for row in rows]
        self._all_declaration_names = [row[1] or "" for row in rows]

        # BM25 on names (spaced and raw tokenization)
        # Use list(set(...)) to dedupe tokens
        names = self._all_declaration_names
        corpus_name_spaced = [list(set(_tokenize_spaced(n))) for n in names]
        corpus_name_raw = [list(set(_tokenize_raw(n))) for n in names]

        self._bm25_name_spaced = bm25s.BM25(method="bm25+")
        self._bm25_name_spaced.index(corpus_name_spaced)

        self._bm25_name_raw = bm25s.BM25(method="bm25+")
        self._bm25_name_raw.index(corpus_name_raw)

        num_declarations = len(self._all_declaration_ids)
        logger.info(f"BM25 indices built over {num_declarations} declarations")

    async def search(
        self,
        query: str,
        limit: int = 50,
        faiss_k: int = 1000,
        bm25_k: int = 1000,
        rerank_top: int | None = 25,
    ) -> list[SearchResult]:
        """Search for Lean declarations using Reciprocal Rank Fusion.

        Two-signal approach:
        1. BM25+ on declaration names (lexical match)
        2. Semantic search on informalizations (meaning match)

        Combined via RRF: score = 1/name_rank + 1/informal_rank

        Optionally applies cross-encoder reranking to the top candidates.

        Args:
            query: Search query string.
            limit: Maximum number of results to return. Defaults to 50.
            faiss_k: Number of candidates from FAISS index. Defaults to 1000.
            bm25_k: Number of candidates from BM25 index. Defaults to 1000.
            rerank_top: If set, apply cross-encoder reranking to top N candidates.
                Set to 0 or None to skip reranking.

        Returns:
            List of SearchResult objects, ranked by combined score.
        """
        if not query.strip():
            return []

        # === STAGE 1: Retrieve candidates from two sources ===

        # 1A: BM25+ on names
        await self._ensure_bm25_loaded()

        query_tokens_spaced = _tokenize_spaced(query)
        query_tokens_raw = _tokenize_raw(query)

        results_name_spaced, scores_name_spaced = self._bm25_name_spaced.retrieve(
            [query_tokens_spaced], k=bm25_k
        )
        results_name_raw, scores_name_raw = self._bm25_name_raw.retrieve(
            [query_tokens_raw], k=bm25_k
        )

        # Build BM25 name map from both tokenizations (take max score)
        bm25_name_map: dict[int, float] = {}
        for idx, score in zip(results_name_spaced[0], scores_name_spaced[0]):
            decl_id = self._all_declaration_ids[idx]
            bm25_name_map[decl_id] = max(bm25_name_map.get(decl_id, 0.0), float(score))
        for idx, score in zip(results_name_raw[0], scores_name_raw[0]):
            decl_id = self._all_declaration_ids[idx]
            bm25_name_map[decl_id] = max(bm25_name_map.get(decl_id, 0.0), float(score))

        logger.info(f"BM25 name: {len(bm25_name_map)} candidates")

        # 1B: Semantic search on informalizations
        embedding_response = await self.embedding_client.embed([query], is_query=True)
        query_embedding = np.array(
            [embedding_response.embeddings[0]], dtype=np.float32
        )

        import faiss as faiss_module
        faiss_module.normalize_L2(query_embedding)

        informal_index = self.faiss_informal_index
        informal_id_map = self.faiss_informal_id_map
        # Set nprobe for IVF index (controls recall vs speed tradeoff)
        # Higher nprobe = more clusters searched = better recall
        if hasattr(informal_index, "nprobe"):
            informal_index.nprobe = 64
        distances, indices = informal_index.search(query_embedding, faiss_k)

        semantic_informal_map: dict[int, float] = {}
        for idx, dist in zip(indices[0], distances[0]):
            if idx == -1 or idx >= len(informal_id_map):
                continue
            decl_id = informal_id_map[idx]
            sim = float(dist)
            semantic_informal_map[decl_id] = max(
                semantic_informal_map.get(decl_id, 0.0), sim
            )

        logger.info(f"FAISS informal: {len(semantic_informal_map)} candidates")

        # === STAGE 2: Compute RRF scores with dependency boost ===
        # RRF: score = 1/name_rank + 1/informal_rank
        # Then boost by how many other candidates depend on each declaration

        all_candidate_ids = (
            set(bm25_name_map.keys()) | set(semantic_informal_map.keys())
        )
        logger.info(f"Total merged candidates: {len(all_candidate_ids)}")

        if not all_candidate_ids:
            return []

        # Sort each signal by score descending to get ranks
        bm25_sorted = sorted(bm25_name_map.items(), key=lambda x: x[1], reverse=True)
        sem_inf_sorted = sorted(
            semantic_informal_map.items(), key=lambda x: x[1], reverse=True
        )

        # Build rank maps (1-indexed)
        bm25_rank_map = {cid: rank + 1 for rank, (cid, _) in enumerate(bm25_sorted)}
        sem_inf_rank_map = {
            cid: rank + 1 for rank, (cid, _) in enumerate(sem_inf_sorted)
        }

        # Default rank for missing candidates
        default_bm25_rank = len(bm25_sorted) + 1
        default_sem_inf_rank = len(sem_inf_sorted) + 1

        # Compute initial RRF scores
        rrf_scores: list[tuple[int, float]] = []
        for cid in all_candidate_ids:
            name_rank = bm25_rank_map.get(cid, default_bm25_rank)
            inf_rank = sem_inf_rank_map.get(cid, default_sem_inf_rank)
            rrf_score = 1.0 / name_rank + 1.0 / inf_rank
            rrf_scores.append((cid, rrf_score))

        # Sort by RRF score descending
        rrf_scores.sort(key=lambda x: x[1], reverse=True)

        # Load top 500 candidates to compute dependency boost
        top500_ids = [cid for cid, _ in rrf_scores[:500]]
        async with AsyncSession(self.engine) as session:
            stmt = select(Declaration).where(Declaration.id.in_(top500_ids))
            result = await session.execute(stmt)
            top500_map = {d.id: d for d in result.scalars().all()}

        # Count dependencies: how many candidates depend on each declaration
        candidate_names = {
            top500_map[cid].name for cid in top500_ids if cid in top500_map
        }
        name_to_id = {
            top500_map[cid].name: cid for cid in top500_ids if cid in top500_map
        }
        dep_counts: dict[int, int] = {cid: 0 for cid in top500_ids}

        for cid in top500_ids:
            decl = top500_map.get(cid)
            if decl and decl.dependencies:
                try:
                    deps = json.loads(decl.dependencies)
                    for dep_name in deps:
                        if dep_name in name_to_id:
                            dep_counts[name_to_id[dep_name]] += 1
                except json.JSONDecodeError:
                    pass

        # Apply dependency boost to RRF scores using RRF formula
        # Final = 1/rrf_rank + 1/dep_rank (treating dep_count as inverse rank)
        max_deps = max(dep_counts.values()) if dep_counts else 0
        boosted_scores: list[tuple[int, float]] = []

        for rank, (cid, rrf_score) in enumerate(rrf_scores[:500], 1):
            dep_count = dep_counts.get(cid, 0)
            # Convert dep_count to a rank (more deps = lower rank = better)
            # dep_rank = 1 for max_deps, higher for fewer deps
            if max_deps > 0 and dep_count > 0:
                dep_rank = (max_deps - dep_count) + 1
            else:
                dep_rank = max_deps + 1 if max_deps > 0 else 501

            # RRF-style combination
            boosted_score = 1.0 / rank + 1.0 / dep_rank
            boosted_scores.append((cid, boosted_score))

        # Re-sort by boosted scores
        boosted_scores.sort(key=lambda x: x[1], reverse=True)
        logger.info("Applied dependency boost to top 500 candidates")

        # Take top candidates for reranking or final output
        top_n = rerank_top if rerank_top and rerank_top > 0 else limit
        top_candidate_ids = [cid for cid, _ in boosted_scores[:top_n]]

        # Use already loaded declarations
        candidates_map = {
            cid: top500_map[cid] for cid in top_candidate_ids if cid in top500_map
        }

        # Build ordered results
        scored_results: list[tuple[Declaration, float]] = [
            (candidates_map[cid], score)
            for cid, score in boosted_scores[:top_n]
            if cid in candidates_map
        ]

        # === STAGE 3: Cross-encoder reranking + fuzzy name match ===
        if rerank_top and rerank_top > 0:
            logger.info(f"Reranking top {len(scored_results)} candidates")

            # Rerank name + informalization for semantic matching
            info_documents = [
                f"{decl.name}: {decl.informalization}"
                if decl.informalization
                else decl.name
                for decl, _ in scored_results
            ]

            rerank_response = await self.reranker_client.rerank(query, info_documents)
            reranker_scores = rerank_response.scores

            # Compute fuzzy name match scores for candidates
            # Only candidates with score >= 0.3 get a fuzzy rank boost
            fuzzy_name_scores = [
                (i, _fuzzy_name_score(query, decl.name))
                for i, (decl, _) in enumerate(scored_results)
            ]
            fuzzy_score_map = {i: score for i, score in fuzzy_name_scores}

            # BM25+ on informalizations for lexical matching
            informalizations = [
                decl.informalization if decl.informalization else decl.name
                for decl, _ in scored_results
            ]
            # Tokenize informalizations (simple word tokenization)
            informal_tokens = [
                [w.lower() for w in re.findall(r"\w+", text)]
                for text in informalizations
            ]
            query_tokens_informal = [w.lower() for w in re.findall(r"\w+", query)]

            # Build BM25 index on informalizations
            bm25_informal = bm25s.BM25(method="bm25+")
            bm25_informal.index(informal_tokens)
            results_informal, scores_informal = bm25_informal.retrieve(
                [query_tokens_informal], k=len(informal_tokens)
            )

            # Build BM25 informal scores list
            bm25_informal_scores = [0.0] * len(scored_results)
            for idx, score in zip(results_informal[0], scores_informal[0]):
                if int(idx) < len(bm25_informal_scores):
                    bm25_informal_scores[int(idx)] = float(score)

            # Compute dependency counts: how many candidates depend on each
            candidate_names = {decl.name for decl, _ in scored_results}
            dep_counts_map: dict[str, int] = {name: 0 for name in candidate_names}

            for decl, _ in scored_results:
                if decl.dependencies:
                    try:
                        deps = json.loads(decl.dependencies)
                        for dep_name in deps:
                            if dep_name in dep_counts_map:
                                dep_counts_map[dep_name] += 1
                    except json.JSONDecodeError:
                        pass

            # Build dep counts list in same order as scored_results
            dep_counts_list = [
                dep_counts_map.get(decl.name, 0) for decl, _ in scored_results
            ]

            # Normalize all signals to [0, 1] range
            norm_reranker = _normalize_scores(reranker_scores)
            norm_fuzzy = _normalize_scores(
                [fuzzy_score_map[i] for i in range(len(scored_results))]
            )
            norm_bm25 = _normalize_scores(bm25_informal_scores)
            norm_dep = _normalize_deps_log(dep_counts_list)

            # Combine with weights:
            # reranker + fuzzy (if >= 0.7) + dep_boost + bm25
            final_scores = []
            for i, (decl, _) in enumerate(scored_results):
                score = 1.0 * norm_reranker[i] + 0.4 * norm_bm25[i] + 0.2 * norm_dep[i]
                # Only add fuzzy boost if raw score meets threshold
                if fuzzy_score_map[i] >= 0.7:
                    score += 1.0 * norm_fuzzy[i]
                final_scores.append(score)

            # Sort by final scores
            combined = sorted(
                zip(scored_results, final_scores),
                key=lambda x: x[1],
                reverse=True,
            )

            # Filter out auto-generated declarations and return
            results = []
            for (decl, _), _ in combined:
                if not _is_autogenerated(decl.name):
                    results.append(self._to_search_result(decl))
                    if len(results) >= limit:
                        break
            return results

        # Filter out auto-generated declarations and return
        results = []
        for decl, _ in scored_results:
            if not _is_autogenerated(decl.name):
                results.append(self._to_search_result(decl))
                if len(results) >= limit:
                    break
        return results

    async def get_by_id(self, declaration_id: int) -> SearchResult | None:
        """Retrieve a declaration by ID.

        Args:
            declaration_id: The declaration ID.

        Returns:
            SearchResult if found, None otherwise.
        """
        async with AsyncSession(self.engine) as session:
            decl = await session.get(Declaration, declaration_id)
            return self._to_search_result(decl) if decl else None

    async def get_by_name(self, name: str) -> SearchResult | None:
        """Retrieve a declaration by its exact name.

        Args:
            name: The exact declaration name (e.g., "AlgebraicGeometry.Scheme").

        Returns:
            SearchResult if found, None otherwise.
        """
        async with AsyncSession(self.engine) as session:
            stmt = select(Declaration).where(Declaration.name == name)
            result = await session.execute(stmt)
            decl = result.scalar_one_or_none()
            return self._to_search_result(decl) if decl else None

    def _to_search_result(self, decl: Declaration) -> SearchResult:
        """Convert Declaration ORM object to SearchResult.

        Args:
            decl: Declaration ORM object.

        Returns:
            SearchResult pydantic model.
        """
        return SearchResult(
            id=decl.id,
            name=decl.name,
            module=decl.module,
            docstring=decl.docstring,
            source_text=decl.source_text,
            source_link=decl.source_link,
            dependencies=decl.dependencies,
            informalization=decl.informalization,
        )
